Programs:

Read all movie files, and write out master files:
  1)  ALL_MOVIES sorted by movie.
  2)  ALL_USERS sorted by user.

movie,user,rating,date
user,movie,rating,date

Convert dates into days since the date 1/1/2000 do have a method to compare dates.

days_since_2000.cc     -- test bench to convert date to number of days since 1/1/2000
generate_all_movies.cc -- read every movie file and store into one file called ALL_MOVIES as above


Binary format of file:

movie,user,rating,days
uint16_t,uint32_t,uint8_t,uint16_t

TODO:

** Note in my paper all was implemented from scratch in C++. **

3-4x over Perl for UBCF.  Wish I had more time to optimize.

For tomorrow, get knn users along with # movies each matched above certain thresholds.
Get some RMSE numbers! 

Cosine is done!
Pearson is done!

Get IBCF working
  - Cosine
  - Pearson
  
Experiment with blending of global averages to low matching counts.

Blend priority:  IBCF vs. UBCF vs. Global Averages
   -- Slope-one
   -- Average blending (possibly weighted).

Normalized vs. non-normalized.

Get k-fold working.

*** Stretch goal is to include time information.


!!!! Measurements !!!!

Record what I learn along the way!!!!!

!!!! Measurements !!!!

Definition:  pure cold start == very first movie watched

For UBCF of pure cold start:  RMSE = 1.15591  (371/5500 ratings) 6.75% of users
Overall data set of pure cold starts:  0.2% -- Reasonable to remove some of the pure cold starts
This might not be easy to do.

However, will next try stratified sampling by number of ratings.  Constructing frequency count of
movie ratings to see if I can break into categories.

Decided to use this as my buckets.  For 2000 count the breakdown is:

Bucket 0 = 1765
Bucket 1 = 133
Bucket 2 = 56
Bucket 3 = 29
Bucket 4 = 17

>>> Buckets = 5
Bucket 0 0 < bucket <= 9650 ( rating count = 20100658 ) [ movies in bucket = 15680 ] [ percent composition = 0.882386 ] 
Bucket 1 9650 < bucket <= 28610 ( rating count = 20097720 ) [ movies in bucket = 1184 ] [ percent composition = 0.0666291 ] 
Bucket 2 28610 < bucket <= 58563 ( rating count = 20121185 ) [ movies in bucket = 495 ] [ percent composition = 0.0278559 ] 
Bucket 3 58563 < bucket <= 101500 ( rating count = 20151525 ) [ movies in bucket = 259 ] [ percent composition = 0.0145751 ] 
Bucket 4 101500 < bucket <= 232944 ( rating count = 20009419 ) [ movies in bucket = 152 ] [ percent composition = 0.00855374 ]

UBCF sucks ass!

Is normalization that important?

Despair -- :( :( :(

Only hope if that KNN > 1 gives better results with an average.  If so, hope alive, otherwise F!

Looks like training set is actually too large!

Item based collaborative filtering might be the way to go.

==> If low density doesn't work then see if I can generate a sampled set that makes high density data.

!!!!! Check programming, something isn't quite right.  Seems like UBCF is a fringe benefit only, not a main way of doing things.

UBCF and IBCF do not work well at very low densities (the 2.0% to 2.5% range appears to be the switch-over point.
The stratified sampling might be wasted, at least directly.

-- Goals:
  Generate a high density item and movie

*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======

This could demonstrate the bane of sparsity and how it should be ignored?!

Instead cull dataset for top M users and top N items.  This should produce better results for IBCF and UBCF.

*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======
*!@&%#($(#@$ =====>  Try to instead sample from the top X users and top Y items!   <======

From the paper:

"In cases requiring real-time computation, the user average is the best algorithm, except
for a small region where item average is preferred."
1. When there are no computation constraints, the conclusions from the previous sections
apply. Specifically, NMF performs best for sparse dataset, BPMF performs best for
dense dataset, and regularized SVD and PMF perform the best otherwise (PMF works
well with smaller user count while Regularized SVD works well smaller item counts).
2. When the time constraint is 5 minutes, Regularized SVD, NLPMF, NPCA, and Rankbased
CF (the ones colored darkly in Figure 11) are not considered. In this setting,
3. When the time constraint is 1 minutes, PMF and BPMF are additionally excluded
from consideration. Slope-one works best in most cases, except for the sparsest data
where NMF works best.
4. In cases requiring real-time computation, the user average is the best algorithm, except
for a small region where item average is preferred.