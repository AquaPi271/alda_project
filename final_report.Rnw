\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{The Netflix Prize:  Exploration of High Density in Recommender Systems in a Large Dataset}


\author{
Timothy Ozimek\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
304 Fincastle Drive\\
Cary, NC 27513
\texttt{teozimek@ncsu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

%\begin{abstract}
%The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
%right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
%The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
%line spaces precede the abstract. The abstract must be limited to one
%paragraph.
%\end{abstract}

\section{Background}

The movie service, Netflix, offers customers the choice of watching several thousand movies on demand.  While the service now offers a highly popular online, on-demand, streaming delivery method, in the past, operations were run through a mail DVD system.  A user of the service could request a certain number of movies at any given time.  After watching and returning the DVDs, a customer could receive new movies as quickly as the mail service could deliver them.  Along with the movies, Netflix provided a list of recommended movies based on a user's viewing preferences.  A viewer can voluntarily rate movies they've seen with the understanding that their rating would guide Netflix in suggesting new movies.  The more ratings they gave, the better the recommendations became.

To provide this service, Netflix wrote the software tool, {\it Cinematch }[1].  This tool used a combination of machine learning and data mining algorithms to perform its duties.  It uses the aptly named recommender systems, which have received serious study over the years.  The intuition behind these systems is based on how word-of-mouth conversations spread news of a product.  These conversations occur between friends and acquaintances who tend to share common opinions over a variety of topics.  Likes and dislikes accordingly tend to correlate.  

The field of data mining and machine learning has observed this relationship and formalized the task in a recommender system.  Such systems tend to employ the technique of collaborative filtering.  In short, this method scours data, looking for similarities in rating patterns, much like word-of-mouth works among similar friends, but at a far larger scale.  The result of this similarity search will filter a group of similar users.  This filtered group provides numerical information to compute a recommendation[3].

A variety of algorithms have been adopted. Two stand large: item-based collaborative filtering (IBCF) and user-based collaborative filtering (UBCF).  The names of each imply their method of search.  IBCF finds similarity of items, in this case movies, and makes recommendations based on the ratings of a target user.  Likewise UBCF establishes similarity through the intuitive word-of-mouth approach by comparing similar user ratings[2].

To stimulate research in improving their recommender system, Netflix offered the Netflix prize in 2006.  The objective seemed tantalizingly simple: be the first team to improve recommendations by 10\% over {\it Cinematch } and receive \$1 million.  Without delving too far into details, contestants were given a dataset of over 100 million user ratings of 17700 movies.  Against this dataset algorithms could be devised, tested, and submitted to Netflix.  Netflix would run the submission against a hidden testset.  A simple root-mean-square-error would be computed per tested rating to arrive at a final score.  This score was the metric compared against {\it Cinematch } to determine the winner[1].

The prize was captured in September 2009 by the team "BellKor's Pragmatic Chaos", a group of professional statisticians and machine learning experts.  Their methods were sophisticated, entirely new, and were comprised of 107 predictor algorithms run as an ensemble. At their core, however, they were recommender systems that had advanced ways of producing better similarity metrics[3].

The objective of this paper is less ambitious than the Netflix Prize.  The combined efforts of the winning team included well over 2000 hours of work for their preliminary solution where even their simplest algorithms took 45 minutes to run. Several searches spanned over many hours[4].  The scope is beyond what can be provided in less than a semester's time.  

Instead this paper will examine a high density subset of the dataset.  Initial exploration of dataset included the use of the {\it recommenderlab} tool suite, a set of tools and algorithms in R "which provides the infrastructure to develop and test recommender algorithms for rating data and 0-1 data in a unified framework"[5].  While the tool was useful for exploring small datasets it did not scale with high memory / density matrices and therefore was abandoned.  Instead a custom, optimized C++11 library of tools was developed from scratch to measure:  UBCF, IBCF, similarity, normalization, nearest neighbor optimizations, and finding where high density based predictions begin to lose effectiveness.

High density simply means that the user-item ratings matrix contains enough entries to fill the matrix past a pre-determined percentage.  The study [3] explores performance for different collaborative filtering algorithms especially towards the lower end of the density spectrum.  It notes that depending on density different approaches should be taken to get collaborative filtering to work.

The objective of this paper is to explore the high density region ($>$ 20 \%) with user-based collaborative filtering, item-based collaborative filtering, different methods of similarity comparison, finding 'k' nearest neighbor parameters, comparing the effects of normalization, finding if UBCF or IBCF trends better, and establishing a relationship between accuracy and various levels of density in this high density region.

\section{Method}

The approach to the project is essentially a survey of different recommender systems.  The kernel of these approaches are the two main collaborative filtering techniques:  user and item based.  Two main programs run the algorithms of each while several others will construct appropriate input datasets.  Parameters for normalization, similarity, dataset, and nearest neighbors will be supplied to each.  Below is a discussion of each method.

\subsection{UBCF}

This technique is given a set of test users that request ratings for a randomly selected movie of which they have rated.  Against this training set, all users who have rated the test movie are extracted.  In this subset of users a similarity comparison is done against the test user.  A sorted list is returned.  From the most similar users in this list, as set by a parameter $k$, a rating can be made by averaging results of their scores.  The search is expensive because there is no easy way of visiting the entire user space, especially with a dense subset.  Because this technique compares user-to-user directly it receives the UBCF designation.

\subsection{IBCF}

Similar to UBCF, IBCF instead seeks items that are most similar to each other.  In most traditional uses of IBCF, a list of most similar items are returned based on a similarity score.  The algorithm has been modified to instead return a user's estimated rating from a similarity matrix.

The first step is to construct a user / movie ratings matrix where all ratings of the training set are filled accordingly.  Then, systematically, each item's ratings are compared to all other items.  A similarity computation is performed that yields a similarity number.  This number is stored in a user / movie similarity matrix.  Note that this matrix is triangular and will be roughly half the size of the ratings matrix.

When a test user requests a rating, the test movie's row index is used.  Along this row are returned the similarity values for all of the test user's known ratings.  A weighted sum of the user's own ratings is used via the formula to construct the final result:

$$ \textrm{Rating} = \frac{\sum_{i=1}^k( r_i * sim_i )}{\sum_{i=1}^k(sim_i)} $$

Another weighing explored was the squared sum.  The highest similarity is always 1 therefore the lower the similarity the more the squared rating is penalized:

$$ \textrm{Rating} = \frac{\sum_{i=1}^k( r_i * sim_i^2 )}{\sum_{i=1}^k(sim_i^2)} $$

\subsection{Similarity Metrics:  Cosine and Pearson}

Similarity metrics in this paper follow two main formulations.  Cosine distance measures similarity via the formula:

$$ cos(\theta) = similarity(A,B) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^N A_i B_i}{\sqrt{\sum_{i=1}^N A_i^2}\sqrt{\sum_{i=1}^N B_i^2}} $$

Pearson similarity is computed as:

$$ r = similarity(A,B) = \frac{\sum_{i=1}^N(A_i - \bar{A})(B_i - \bar{B})}{\sqrt{\sum_{i=1}^N(A_i - \bar{A})^2}\sqrt{\sum_{i=1}^N(B_i - \bar{B})^2}}$$

Both forms will be examined across UBCF and IBCF.  The Pearson is expensive because it requires two passes of the data, one for the average, and the second for the similarity.  However, this cost may be justified because of an inherent tendency to normalize.

\subsection{Normalization}

Users can be optimists or pessimists on all their ratings or for portions of their ratings.  To see the effect of this bias, similarity measurements will be conducted under normalized and non-normalized data.  In UBCF two main biases will be normalized.  The first will be for computation of similarity.  The second will be on the returned score to the user, to shift back into the biased domain.

For IBCF, the similarity matrix is constructed with normalized ratings.  Because only a similarity is returned, a test user's bias need not be computed.

The normalization formula used is simply a shifted bias from the average rating a user gave on all movies that they rated:

$$ Rating_i = Rating_{ui} - \overline{Rating_u} $$

\subsection{Blending of Global Data for Cold Starts or Sparse Matches}

In cases where similarity yields undefined results, for example, floating point overflow or underflow, the average rating for a movie is used instead.  Care is taken to check for such conditions.

\subsection{Other Approaches Considered}

There are many other areas to explore with recommender systems.  Some mentioned in [3], [4], [6], and [7] include Restricted Boltzmann Machines (RBM), Matrix Factorization / Single Value Decomposition (SVD), and Slope-One estimation.  Even more exotic approaches described in [2] include simulated annealing.  Indeed, the space of applications is very large and continually growing.  Computation and limited research time prevented exploration into these areas.  [1] indicates that the Netflix Prize \underline{intermediate} solution took 2000 hours and involved 107 algorithms for RBM and SVD, which exceeded the time alloted for the assignment.  The Slope-One estimation was more appropriate for linear regressions but not with the implemented system in this paper.  Finally, only the high density region of the dataset was explored.  The paper [7] notes that different approaches might be considered depending on the data density with higher density producing more reliable results.  The results of this project might show an upper bound expected for these types of algorithms. 

\section{Plan and Experiment}

\subsection{Dataset}

The dataset is comprised of over 100 million ratings of 17,700 movies from 480,189 users.  17,700 text files, one per movie, contain a user id, rating, and date per line in ASCII comma-separated format.  A movie ID value was given on the first line of the file as an integer number.  A separate file gives the movie title for a given ID.  User IDs were arbitrary integers assigned by Netflix that stripped away personally identifying information.  The same ID is used by a user for all movies that they rate.  Ratings are ordinal integer values from 1 to 5, with 1 indicating the worst rating and 5 the best.  Dates are also given in month-day-year format.  

Of the above data, only the movie ID, user ID, and ratings are used in this paper.  No testing sets are provided in the original data and are generated which is described in subsequent section.  Auxilliary programs convert all relevant data into one master file in binary format to conserve on space and load times.  Subsampled data is also in this format.

\subsection{Hypotheses / Questions}

The examination of the dataset will attempt to answer the follow questions.

\begin{itemize}

\item This paper posits that the ideal $k$ parameter in the $k$ nearest neighbors algorithm will vary significantly between IBCF and UBCF and will need to be greater than at least 5 in either case.  Both CF algorithms can return an arbitrary number of similar users.  Cosine and Pearson similarity return scores in the range of -1 to 1.  Values closer to 1 indicate higher similarity.  Finding the ideal $k$ parameter for $k$ nearest neighbors will govern the overall performance of the algorithm.  

\item Between the similarity metrics this paper expects Pearson similarity to outperform Cosine similarity in all regards.  Pearson similarity is a correlation metric that captures global effects by using the average value of each vector.  This can be viewed as a built-in normalization effect.  Furthermore, Pearson against normalized and untreated data will give similar results.

\item Normalization will offer some benefit to both algorithms over untreated data.  However, the effect should be relatively small.  The ratings space only occupies 5 ordinal values; there is not much space where normalization values can settle.

\item High density data will yield superb results for UBCF and IBCF with RMSE results on par with the Netflix Prize winners results (0.8650 RMSE) on the overall dataset.  These are the best conditions for such data.  However, a cross-over point will be observed where average RMSE will beat these metrics.  A simple test against a low density sample showed that RMSE scored 1.05 while the other methods ranged from 1.20 to 1.32 RMSE.  A cross-over point might occur in the 30 \% density region where the results could deteriorate.

\item User based collaborative filtering will work better than item based collaborative filtering.  This is just a guess.  It will be interesting to see which method produces better results.

\item The relationship for a fixed number of movies and users in a subsample may reveal where each trend will hit their respective cross-over points as compared to the baseline metric.  Essentially, this question expects an inverse linear relationship between RMSE per density.

\end{itemize}


% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}


%\begin{table}[ht]
%\caption{Movie vs. user count subsample densities}
%\label{density-table-label}
%\begin{center}
%\begin{tabular}{cp{1cm}cccccc}
%\hline
%& & \multicolumn{5}{l}{User count}\\
%\hline
%\multirow{5}{*}{Movie count} & 500   & 1500  & 2500  & 3500  & 4500  \\ \hline
%1000 & 0.851 & 0.794 & 0.763 & 0.741 & 0.723 \\ \hline
%3000         & 0.624 & 0.541 & 0.502 & 0.476 & 0.456 \\ \hline
%5000         & 0.491 & 0.405 & 0.368 & 0.344 & 0.327 \\ \hline
%7000         & 0.401 & 0.320 & 0.287 & 0.267 & 0.252 \\ \hline
%9000         & 0.338 & 0.263 & 0.234 & 0.217 & 0.204 \\ \hline
%\end{tabular}
%\end{center}
%\end{table}

\subsection{Experimental Design}

The overall experiment will follow the guidelines for evaluation as set by the Netflix Prize.  A test set (not provided) will query the recommender system which will give a decimal rating for the user based on that user's prior movie ratings.  Error measurements are computed by the root-mean-square-error formula:

$$ RMSE = \sqrt{\frac{\sum_{i=1}^N (\hat{y_i} - y_i)^2}{N}} $$

Unlike linear error methods such as MAE, RMSE penalizes errors more severely.  Accuracy is therefore, at a premium.

Subsamples are generated from the highest top $X$ movies and top $Y$ users in terms of raw number of ratings count.  The subsamples will fill a 5 x 5 matrix of varying $X$ and $Y$ counts.

From each subsample, 10-fold cross validation is performed.  For the testing portion of a fold, a test subject will randomly have a rating stripped from the test set.  The rating will be stored separately for later RMSE computation against that user's predicted value. 

Randomness will be controlled through C++11's uniform random number generator with seedings recorded for reproducibility.

The baseline metric will always be the average rating for a movie over the entire training set.  This baseline is used by all citations that analyze Netflix Prize recommender system algorithms.  

The first $k$ nearest-neighbor experiment will determine the $k$ parameter to be used in all subsequent experiments.  All combinations similarity metric, CF algorithm, and normalization will be used to find which best $k$ value to use per setting.  The results will be computed against the (Movie = 1500 x User = 500) subsample with $k$ being the independent variable.

The remaining experiments will use the entire 5 x 5 set of high density subsamples and observe the $k$ parameter discovered above.  Comparisons are between: Cosine and Pearson . Normalize and Untreated Data, and UBCF versus IBCF.

For uncovering user and movie dependencies, either parameter will be fixed while the other varies. 

%

\section{Results}

\subsection{K Nearest Neighbor Parameter Search}

<<label=knn,include=FALSE>>=
library("reshape2")
library("ggplot2")
library("gridExtra")

cf_k_table <- read.delim(text="k UBCF_COSINE_DENORMALIZED IBCF_COSINE_DENORMALIZED UBCF_COSINE_NORMALIZED IBCF_COSINE_NORMALIZED UBCF_PEARSON_DENORMALIZED IBCF_PEARSON_DENORMALIZED UBCF_PEARSON_NORMALIZED IBCF_PEARSON_NORMALIZED
1 1.23655 0.920048 1.11839 0.920048 1.23655 0.920048 1.11839 0.920048
2 1.10926 1.00381 0.888671 0.905941 0.997885 0.878653 0.98337 0.881869
3 1.10323 0.987016 0.850719 0.864337 0.953396 0.83077 0.89544 0.833321
4 1.0765 0.987237 0.817349 0.839805 0.913284 0.810032 0.843541 0.806225
5 1.04544 0.981972 0.805295 0.821832 0.897593 0.799235 0.829648 0.788883
6 1.03094 0.978671 0.786088 0.812109 0.875153 0.797812 0.822661 0.777133
7 1.01337 0.97742 0.779532 0.808517 0.86764 0.790032 0.804924 0.777174
8 1.00257 0.981971 0.775688 0.80096 0.870702 0.785709 0.798699 0.774489
9 1.00548 0.978952 0.773115 0.798931 0.86799 0.779873 0.793205 0.767587
10 1.00712 0.980134 0.774617 0.795652 0.864337 0.776034 0.791223 0.759063
11 1.01112 0.979088 0.771025 0.792216 0.865305 0.772696 0.783226 0.756642
12 1.00416 0.977 0.772224 0.787178 0.868169 0.771713 0.775699 0.758621
13 1.00034 0.977806 0.771314 0.788605 0.863967 0.771328 0.772654 0.757013
14 0.997866 0.977861 0.769123 0.788509 0.862327 0.767712 0.776436 0.757467
15 0.994801 0.976343 0.766873 0.787973 0.864196 0.769112 0.779886 0.757156
16 0.991728 0.97521 0.764871 0.78885 0.862905 0.769519 0.777933 0.759028
17 0.988238 0.976085 0.763372 0.78805 0.861703 0.771228 0.775126 0.759139
",as.is=TRUE,sep=" ",header=TRUE)

p1 <- ggplot(cf_k_table, aes(k)) +
  geom_point(aes(y=UBCF_PEARSON_NORMALIZED, colour = "UBCF\nPEARSON")) +
  geom_line(aes(y=UBCF_PEARSON_NORMALIZED, colour = "UBCF\nPEARSON"), size=1) +
  geom_point(aes(y=IBCF_PEARSON_NORMALIZED, colour = "IBCF\nPEARSON")) +
  geom_line(aes(y=IBCF_PEARSON_NORMALIZED, colour = "IBCF\nPEARSON"), size=1) +
  labs(y = "RMSE") +
  theme(panel.background = element_blank())

p2 <- ggplot(cf_k_table, aes(k)) +
  geom_point(aes(y=UBCF_COSINE_NORMALIZED, colour = "UBCF\nCOSINE")) +
  geom_line(aes(y=UBCF_COSINE_NORMALIZED, colour = "UBCF\nCOSINE"), size=1) +
  geom_point(aes(y=IBCF_COSINE_NORMALIZED, colour = "IBCF\nCOSINE")) +
  geom_line(aes(y=IBCF_COSINE_NORMALIZED, colour = "IBCF\nCOSINE"), size=1) +
  labs(y = "RMSE") +
  theme(panel.background = element_blank())
  #theme(panel.background = element_blank(), legend.position = "none")

grid.arrange(p1,p2,padding=10,ncol=2)
@


\begin{figure}[!ht]
\begin{center}
<<knn_plot,ref.label='knn', fig.width=6.0, fig.height=2.0, out.width='.75\\linewidth', echo=FALSE, crop=TRUE>>=
@
\end{center}
\caption{$K$ nearest neighbor parameter search}
\label{fig:knn}
\end{figure}

At lower values, $k$ is susceptible to one or two dominating nearest neighbors who may overemphasize certain traits at the expense of others.  In effect, they become outliers.

At the higher extreme, the $k$ parameter saturates RMSE scores.  This is because as more neighbors are added the less they becomed differentiated from each other.  Thus, the differences in similarity start offering diminishing returns.  Consider that if all elements are returned into the dataset that the estimated rating would become the overall average rating.  This implies the the $k$ curve will eventually worsen over time rather than incrementally improve.

The above graphs are examples of the overall trend in $k$ values.  An adequate location for $k$ appears in the 8 to 12 region.  All remaining experiments set the value to 10.

\subsection{UBCF and IBCF Similarity and Normalization Comparisons}

<<label=cf_compare,include=FALSE>>=
library("reshape2")
library("ggplot2")
library("gridExtra")

ubcf_compare_table <- read.delim(text="Measurement,Base_Line,Cosine_Denorm,Cosine_Norm,Pearson_Denorm,Pearson_Norm,
1,1.0797,1.19458,0.886032,0.958599,0.93466,
2,1.11195,1.18205,0.831459,1.01042,0.86987,
3,1.07182,1.18017,0.825513,0.974755,0.873476,
4,1.03431,1.16384,0.815974,0.903752,0.858213,
5,1.02696,1.13505,0.861247,0.970767,0.886639,
6,1.03228,1.13174,0.855554,0.967849,0.894999,
7,1.03952,1.11968,0.849884,0.966142,0.88241,
8,1.02817,1.10991,0.847066,0.965589,0.881449,
9,1.01519,1.10254,0.843545,0.944243,0.902373,
10,1.03265,1.10207,0.836399,0.933183,0.867191,
11,1.03867,1.10052,0.856144,0.977383,0.891405,
12,1.02151,1.09749,0.854037,0.975044,0.890092,
13,1.05139,1.09585,0.875231,0.989457,0.918303,
14,1.04383,1.08867,0.848398,0.994004,0.890411,
15,1.00432,1.07522,0.822458,0.923591,0.856352,
16,1.00754,1.06909,0.86171,0.956693,0.88929,
17,0.991315,1.05956,0.848119,0.942754,0.881036,
18,0.99768,1.05817,0.839758,0.956986,0.870892,
19,0.97684,1.04926,0.826053,0.942877,0.852991,
20,0.989388,1.00991,0.829869,0.947265,0.865501,
21,0.936467,1.00712,0.766873,0.864337,0.779886,
22,0.936927,1.0034,0.831235,0.897187,0.869117,
23,0.972416,0.998826,0.829542,0.920469,0.857756,
24,0.969563,0.97859,0.828095,0.920952,0.86166,
25,0.968466,0.971134,0.816331,0.917372,0.858401
",as.is=TRUE,sep=",",header=TRUE)

p1 <- ggplot(ubcf_compare_table, aes(Measurement)) +
  geom_point(aes(y=Base_Line, colour = "Base\nLine")) +
  geom_line(aes(y=Base_Line, colour = "Base\nLine"), size=1) +
  geom_point(aes(y=Cosine_Denorm, colour = "Cosine\nDenorm")) +
  geom_line(aes(y=Cosine_Denorm, colour = "Cosine\nDenorm"), size=1) +
  geom_point(aes(y=Cosine_Norm, colour = "Cosine\nNorm")) +
  geom_line(aes(y=Cosine_Norm, colour = "Cosine\nNorm"), size=1) +
  geom_point(aes(y=Pearson_Denorm, colour = "Pearson\nDenorm")) +
  geom_line(aes(y=Pearson_Denorm, colour = "Pearson\nDenorm"), size=1) +
  geom_point(aes(y=Pearson_Norm, colour = "Pearson\nNorm")) +
  geom_line(aes(y=Pearson_Norm, colour = "Pearson\nNorm"), size=1) +
  labs(x = "UBCF Measurements", y = "RMSE") +
  theme(panel.background = element_blank())


ibcf_compare_table <- read.delim(text="Measurement,Base_Line,Cosine_Denorm,Cosine_Norm,Pearson_Denorm,Pearson_Norm,
1,1.11195,0.897058,0.902804,1.02782,0.925272,
2,1.0797,0.915498,0.8813,0.957619,0.893324,
3,1.07182,0.925161,0.903281,0.987451,0.884216,
4,1.05139,0.938165,0.860096,0.930309,0.832674,
5,1.04383,0.921395,0.881775,0.948156,0.866116,
6,1.03952,0.935607,0.871306,0.940676,0.843588,
7,1.03867,0.913476,0.853248,0.869857,0.80264,
8,1.03431,0.915701,0.863473,0.842907,0.814354,
9,1.03265,0.913542,0.890082,0.970211,0.859913,
10,1.03228,0.921004,0.849889,0.908046,0.821073,
11,1.02817,0.91534,0.868075,0.964685,0.846731,
12,1.02696,0.92359,0.856333,0.871303,0.806911,
13,1.02151,0.938716,0.861714,0.901069,0.824488,
14,1.01519,0.924627,0.871233,0.926576,0.848432,
15,1.00754,0.956687,0.85666,0.813792,0.786609,
16,1.00432,0.90319,0.838938,0.881,0.809491,
17,0.99768,0.92892,0.842843,0.845382,0.799225,
18,0.991315,0.951797,0.832384,0.820826,0.789839,
19,0.989388,0.910055,0.839374,0.817688,0.800846,
20,0.97684,0.963492,0.847452,0.814167,0.79057,
21,0.972416,1.0328,0.830427,0.79425,0.78942,
22,0.969563,1.00172,0.846788,0.800941,0.794451,
23,0.968466,0.993165,0.849096,0.805237,0.803849,
24,0.936927,0.977,0.795652,0.771713,0.759063,
25,0.936467,0.961662,0.754128,0.776903,0.755628
",as.is=TRUE,sep=",",header=TRUE)

p2 <- ggplot(ibcf_compare_table, aes(Measurement)) +
  geom_point(aes(y=Base_Line, colour = "Base\nLine")) +
  geom_line(aes(y=Base_Line, colour = "Base\nLine"), size=1) +
  geom_point(aes(y=Cosine_Denorm, colour = "Cosine\nDenorm")) +
  geom_line(aes(y=Cosine_Denorm, colour = "Cosine\nDenorm"), size=1) +
  geom_point(aes(y=Cosine_Norm, colour = "Cosine\nNorm")) +
  geom_line(aes(y=Cosine_Norm, colour = "Cosine\nNorm"), size=1) +
  geom_point(aes(y=Pearson_Denorm, colour = "Pearson\nDenorm")) +
  geom_line(aes(y=Pearson_Denorm, colour = "Pearson\nDenorm"), size=1) +
  geom_point(aes(y=Pearson_Norm, colour = "Pearson\nNorm")) +
  geom_line(aes(y=Pearson_Norm, colour = "Pearson\nNorm"), size=1) +
  labs(x = "IBCF Measurements", y = "RMSE") +
  theme(panel.background = element_blank())

grid.arrange(p1,p2,padding=10,ncol=2)
@


\begin{figure}[!ht]
\begin{center}
<<compare_plot,ref.label='cf_compare', fig.width=6.0, fig.height=3.0, out.width='1.00\\linewidth', echo=FALSE, crop=TRUE>>=
@
\end{center}
\caption{Collaborative filtering normalization and similarity comparison}
\label{fig:knn}
\end{figure}

Over all combination of the 25 measurements from the 5 x 5 density matrix overall similarity and normalization techniques can be compared to the baseline.  In the UBCF case, normalization squarely outperforms its untreated counterpart by significant margins.  The Cosine Denormalized metric is worse than even the baseline.  In the IBCF the trend is similar.  Overall normalization offers anywhere from 10\% to 20\% RMSE improvement over the baseline.  This relatively simple technique demonstrates that with even 5 ordinal ratings, enough of a consistent bias exists on a per user basis to justify aligning all data to a common unbiased set of data.  This result was signficant and surprising.

The performance of Pearson versus Cosine Normalized is different for UBCF and IBCF with Cosine prevailing in the former and Pearson in the latter.  The expectation was that Pearson would work best in all conditions.  While the improvment is modest it is consistent throughout the experiment.

This data also answer another question.  Raw high density subsamples match or exceed even the Netflix Prize results.  This is fairly unsurprising as much of the dataset is occupied.

-- overall density vs. RMSE (demonstrates no cross-over point in the high density region)

-- UBCF and IBCF parametric graphs (hold one constant while other varies)
 

\section{Conclusion}

The dataset is large.  A good portion of time was spent on exploring IBCF and UBCF features in {\it recommenderlab} by using the built-in MovieLense database.  The routines did a great job in explaining the overall programming approach, but they fail to run against extraordinarily large amounts of data.  The remedy is to sample the full dataset with a collection of C++ tools.  To this end, much of the development work has gone into creating these tools and processing the data for use in R Studio.

The toolsuite might be extended to perform the calculations described in this report.  Raw C++ can handle the large amounts of data and run much quicker than R's interpreted environment.  However, the interpreted environment offers the advantage of operating on data without reloading it for each experiment.

A day or two might be spent looking for APIs to enhance the approach.


KNN IMPORTANCE
NORMALIZATION IMPORTANCE
OPTIMIZATION
GPU
IBCF BETTER THAN UBCF
DENSITY AS LOW AS 20\% WORK WELL POSSIBLY MUCH LOWER
INTERPOLATION METHODS

Lessons learned:  Was overwhelmed by algorithms at first.  Explore a bit more and be patient with results.
(SEE SLIDES)


%The formatting instructions contained in these style files are summarized in
%sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

%% \subsection{Keywords for paper submission}
%% Your NIPS paper can be submitted with any of the following keywords (more than one keyword is possible for each paper):

%% \begin{verbatim}
%% Bioinformatics
%% Biological Vision
%% Brain Imaging and Brain Computer Interfacing
%% Clustering
%% Cognitive Science
%% Control and Reinforcement Learning
%% Dimensionality Reduction and Manifolds
%% Feature Selection
%% Gaussian Processes
%% Graphical Models
%% Hardware Technologies
%% Kernels
%% Learning Theory
%% Machine Vision
%% Margins and Boosting
%% Neural Networks
%% Neuroscience
%% Other Algorithms and Architectures
%% Other Applications
%% Semi-supervised Learning
%% Speech and Signal Processing
%% Text and Language Applications

%% \end{verbatim}

% \section{General formatting instructions}
% \label{gen_inst}
% 
% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.
% 
% Paper title is 17~point, initial caps/lower case, bold, centered between
% 2~horizontal rules. Top rule is 4~points thick and bottom rule is 1~point
% thick. Allow 1/4~inch space above and below title to rules. All pages should
% start at 1~inch (6~picas) from the top of the page.
% 
% %The version of the paper submitted for review should have ``Anonymous Author(s)'' as the author of the paper.
% 
% For the final version, authors' names are
% set in boldface, and each name is centered above the corresponding
% address. The lead author's name is to be listed first (left-most), and
% the co-authors' names (if different address) are set to follow. If
% there is only one co-author, list both author and co-author side by side.
% 
% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.
% 
% \section{Headings: first level}
% \label{headings}
% 
% First level headings are lower case (except for first word and proper nouns),
% flush left, bold and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.
% 
% \subsection{Headings: second level}
% 
% Second level headings are lower case (except for first word and proper nouns),
% flush left, bold and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.
% 
% \subsubsection{Headings: third level}
% 
% Third level headings are lower case (except for first word and proper nouns),
% flush left, bold and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.
% 
% \section{Citations, figures, tables, references}
% \label{others}
% 
% These instructions apply to everyone, regardless of the formatter being used.
% 
% \subsection{Citations within the text}
% 
% Citations within the text should be numbered consecutively. The corresponding
% number is to appear enclosed in square brackets, such as [1] or [2]-[5]. The
% corresponding references are to be listed in the same order at the end of the
% paper, in the \textbf{References} section. (Note: the standard
% \textsc{Bib\TeX} style \texttt{unsrt} produces this.) As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.
% 
% As submission is double blind, refer to your own published work in the 
% third person. That is, use ``In the previous work of Jones et al.\ [4]'',
% not ``In our previous work [4]''. If you cite your other papers that
% are not widely available (e.g.\ a journal paper under review), use
% anonymous author names in the citation, e.g.\ an author of the
% form ``A.\ Anonymous''. 
% 
% 
% \subsection{Footnotes}
% 
% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}
% 
% \subsection{Figures}
% 
% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.
% 
% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.
% 
% You may use color figures. 
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}
% 
% \subsection{Tables}
% 
% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.
% 
% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.
% 
% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}
% 
% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.
% 
% \section{Preparing PostScript or PDF files}
% 
% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.
% 
% Fonts were the main cause of problems in the past years. Your PDF file must
% only contain Type 1 or Embedded TrueType fonts. Here are a few instructions
% to achieve this.
% 
% \begin{itemize}
% 
% \item You can check which fonts a PDF files uses.  In Acrobat Reader,
% select the menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
% also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
% available out-of-the-box on most Linux machines.
% 
% \item The IEEE has recommendations for generating PDF files whose fonts
% are also acceptable for NIPS. Please see
% \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}
% 
% \item LaTeX users:
% 
% \begin{itemize}
% 
% \item Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user). 
% PDF figures must be substituted for EPS figures, however.
% 
% \item Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim} 
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}
% 
% Check that the PDF files only contains Type 1 fonts. 
% %For the final version, please send us both the Postscript file and
% %the PDF file. 
% 
% \item xfig "patterned" shapes are implemented with 
% bitmap fonts.  Use "solid" shapes instead. 
% \item The \verb+\bbold+ package almost always uses bitmap
% fonts.  You can try the equivalent AMS Fonts with command
% \begin{verbatim}
% \usepackage[psamsfonts]{amssymb}
% \end{verbatim}
%  or use the following workaround for reals, natural and complex: 
% \begin{verbatim}
% \newcommand{\RR}{I\!\!R} %real numbers
% \newcommand{\Nat}{I\!\!N} %natural numbers 
% \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% 
% \item Sometimes the problematic fonts are used in figures
% included in LaTeX files. The ghostscript program \verb+eps2eps+ is the simplest
% way to clean such figures. For black and white figures, slightly better
% results can be achieved with program \verb+potrace+.
% \end{itemize}
% \item MSWord and Windows users (via PDF file):
% \begin{itemize}
% \item Install the Microsoft Save as PDF Office 2007 Add-in from
% \url{http://www.microsoft.com/downloads/details.aspx?displaylang=en\&familyid=4d951911-3e7e-4ae6-b059-a2e79ed87041}
% \item Select ``Save or Publish to PDF'' from the Office or File menu
% \end{itemize}
% \item MSWord and Mac OS X users (via PDF file):
% \begin{itemize}
% \item From the print menu, click the PDF drop-down box, and select ``Save
% as PDF...''
% \end{itemize}
% \item MSWord and Windows users (via PS file):
% \begin{itemize}
% \item To create a new printer
% on your computer, install the AdobePS printer driver and the Adobe Distiller PPD file from
% \url{http://www.adobe.com/support/downloads/detail.jsp?ftpID=204} {\it Note:} You must reboot your PC after installing the
% AdobePS driver for it to take effect.
% \item To produce the ps file, select ``Print'' from the MS app, choose
% the installed AdobePS printer, click on ``Properties'', click on ``Advanced.''
% \item Set ``TrueType Font'' to be ``Download as Softfont''
% \item Open the ``PostScript Options'' folder
% \item Select ``PostScript Output Option'' to be ``Optimize for Portability''
% \item Select ``TrueType Font Download Option'' to be ``Outline''
% \item Select ``Send PostScript Error Handler'' to be ``No''
% \item Click ``OK'' three times, print your file.
% \item Now, use Adobe Acrobat Distiller or ps2pdf to create a PDF file from
% the PS file. In Acrobat, check the option ``Embed all fonts'' if
% applicable.
% \end{itemize}
% 
% \end{itemize}
% If your file contains Type 3 fonts or non embedded TrueType fonts, we will
% ask you to fix it. 
% 
% \subsection{Margins in LaTeX}
%  
% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ... 
%    \includegraphics[width=0.8\linewidth]{myfile.eps} 
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ... 
%    \includegraphics[width=0.8\linewidth]{myfile.pdf} 
% \end{verbatim}
% for .pdf graphics. 
% See section 4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps}) 
%  
% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.
% 
% 
% \subsubsection*{Acknowledgments}
% 
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments go at the end of the paper. Do not include 
% acknowledgments in the anonymized submission, only in the 
% final paper. 
% 

\subsection*{GIT Repository}

Code for this project can be found under the 'c++' subdirectory at the Github address \url{https://github.com/AquaPi271/alda_project}.

\subsubsection*{References}

\small{
[1] Xavier Amatriain, Justin Basilico. {\it Netflix Recommendations: Beyond the 5 Stars (Two Parts)}.  The Netflix Tech Blog, 2012. https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429. 11/5/2017.

[2] Linyuan Lu, Matus Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, Tao Zhou. (2012) {\it Recommender systems}. In Physics Reports pp. 1-49, Elsevier B. V. 0370-1573.

[3] Yehuda Koren. (2009). The BellKor Solution to the Netflix Grand Prize.

[4] Robert M. Bell, Yehuda Koren. (2007)  {\it Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights}.  Seventh IEEE International Conference on Data Mining. pp. 43-52. IEEE.

[5] Michael Hahsler (2017). recommenderlab: Lab for Developing and Testing Recommender Algorithms. R package version 0.2-2. http://lyle.smu.edu/IDA/recommenderlab/

[6] Daniel Lemire, Anna Maclachlan. (2007). {\it Slope One Predictors for Online Rating-Based Collaborative Filtering}. In CoRR, Volume abs/cs/0702144.

[7] Joonseok Lee, Mingxuan Sun, Guy Lebanon. (2012).  A Comparative Study of Collaborative Filtering Algorithms.


}
% \subsubsection*{References}
% 
% References follow the acknowledgments. Use unnumbered third level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to `small' (9-point) 
% when listing the references. {\bf Remember that this year you can use
% a ninth page as long as it contains \emph{only} cited references.}
% 
% \small{
% [1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
% for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
% and T.K. Leen (eds.), {\it Advances in Neural Information Processing
% Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.
% 
% [2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
% Realistic Neural Models with the GEneral NEural SImulation System.}
% New York: TELOS/Springer-Verlag.
% 
% [3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
% and recall at excitatory recurrent synapses and cholinergic modulation
% in rat hippocampal region CA3. {\it Journal of Neuroscience}
% {\bf 15}(7):5249-5262.
% }

\end{document}
