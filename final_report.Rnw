\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{The Netflix Prize:  Exploration of High Density in Recommender Systems in a Large Dataset}


\author{
Timothy Ozimek\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
304 Fincastle Drive\\
Cary, NC 27513
\texttt{teozimek@ncsu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

%\begin{abstract}
%The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
%right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
%The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
%line spaces precede the abstract. The abstract must be limited to one
%paragraph.
%\end{abstract}

\section{Background}

The movie service, Netflix, offers customers the choice of watching several thousand movies on demand.  While the service now offers a highly popular online, on-demand, streaming delivery method, in the past, operations were run through a mail DVD system.  A user of the service could request a certain number of movies at any given time.  After watching and returning the DVDs, a customer could receive new movies as quickly as the mail service could deliver them.  Along with the movies, Netflix provided a list of recommended movies based on a user's viewing preferences.  A viewer can voluntarily rate movies they've seen with the understanding that their rating would guide Netflix in suggesting new movies.  The more ratings they gave, the better the recommendations became.

To provide this service, Netflix wrote the software tool, {\it Cinematch }[1].  This tool used a combination of machine learning and data mining algorithms to perform its duties.  It uses the aptly named recommender systems, which have received serious study over the years.  The intuition behind these systems is based on how word-of-mouth conversations spread news of a product.  These conversations occur between friends and acquaintances who tend to share common opinions over a variety of topics.  Likes and dislikes accordingly tend to correlate.  
The field of data mining and machine learning has observed this relationship and formalized the task in a recommender system.  Such systems tend to employ the technique of collaborative filtering.  In short, this method scours data, looking for similarities in rating patterns, much like word-of-mouth works among similar friends, but at a far larger scale.  The result of this similarity search will filter a group of similar users.  This filtered group provides numerical information to compute a recommendation[3].

A variety of algorithms have been adopted. Two stand large: item-based collaborative filtering (IBCF) and user-based collaborative filtering (UBCF).  The names of each imply their method of search.  IBCF finds similarity of items, in this case movies, and makes recommendations based on the ratings of a target user.  Likewise UBCF establishes similarity through the intuitive word-of-mouth approach by comparing similar user ratings[3].

To stimulate research in improving their recommender system, Netflix offered the Netflix prize in 2006.  The objective seemed tantalizingly simple: be the first team to improve recommendations by 10\% over {\it Cinematch } and receive \$1 million.  Without delving too far into details, contestants were given a dataset of over 100 million user ratings of 17700 movies.  Against this dataset algorithms could be devised, tested, and submitted to Netflix.  Netflix would run the submission against a hidden testset.  A simple root-mean-square-error would be computed per tested rating to arrive at a final score.  This score was the metric compared against {\it Cinematch } to determine the winner[1].

The prize was captured in September 2009 by the team "BellKor's Pragmatic Chaos", a group of professional statisticians and machine learning experts.  Their methods were sophisticated, entirely new, and were comprised of 107 predictor algorithms run as an ensemble. At their core, however, they were recommender systems that had advanced ways of producing better similarity metrics[7].

The objective of this paper is less ambitious than the Netflix Prize.  The combined efforts of the winning team included well over 2000 hours of work for their preliminary solution where even their simplest algorithms took 45 minutes to run. Several searches spanned over many hours[2].  The scope is beyond what can be provided in less than a semester's time.  

Instead this paper will examine a high density subset of the dataset.  Initial exploration of dataset included the use of the {\it recommenderlab} tool suite, a set of tools and algorithms in R "which provides the infrastructure to develop and test recommender algorithms for rating data and 0-1 data in a unified framework".[4]  While the tool was useful for exploring small datasets it did not scale with high memory / density matrices and therefore was abandoned.  Instead a custom, optimized C++11 library of tools was developed from scratch to measure:  UBCF, IBCF, similarity, normalization, nearest neighbor optimizations, and finding where high density based predictions begin to lose effectiveness.

High density simply means that the user-item ratings matrix contains enough entries to fill the matrix past a pre-determined percentage.  The study [7] explores performance for different collaborative filtering algorithms especially towards the lower end of the density spectrum.  It notes that depending on density different approaches should be taken to get collaborative filtering to work.

The objective of this paper is to explore the high density region ($>$ 20 \%) with user-based collaborative filtering, item-based collaborative filtering, different methods of similarity comparison, finding 'k' nearest neighbor parameters, comparing the effects of normalization, finding if UBCF or IBCF trends better, and establishing a relationship between accuracy and various levels of density in this high density region.

%It was noted in "BellKor's Pragmatic Chaos"'s work that data sparsity was the main obstacle to address[2].  This project will focus on exploring remedies to this issue by using bulk information from the dataset when insufficient detailed information is in short supply.  When adequate nearest neighbor information is available, the similarity rating will be used.  If the nearest neighbor threshold falls below some value, overall average and variance for movie will be weighted and added to produce the targeted rating.  Timestamps of ratings may also be considered to see if they can be blended into weights to improve target ratings.

%The setup will rely on the {\it recommenderlab } tool suite[4].  The purpose of this suite to to expose a sandbox experimental environment for recommender systems.  Support is provided for UBCF and IBCF systems as well as variants of singular value decomposition (SVD) and hybrid solutions.  

%The dataset also provides timestamp of rating, title of movie, and DVD release date of movie.  Briefly, a bag-of-words approach was considered for identifying movie genres but was rejected outright because of too many proper names.  Additional support will be provided by C++ routines used to clean data, and provide training and test sets. 

%Each contains an ID, date, and rating supplied by a Netflix consumer.  The movie file provides a unique movie ID, release date, and text name for the movie.  

\section{Method}

The approach to the project is essentially a survey of different recommender systems.  The kernel of these approaches are the two main collaborative filtering techniques:  user and item based.  Two main programs run the algorithms of each while several others will construct appropriate input datasets.  Parameters for normalization, similarity, dataset, and nearest neighbors will be supplied to each.  Below is a discussion of each method.

\subsection{UBCF}

This technique is given a set of test users that request ratings for a randomly selected movie of which they have rated.  Against training set, all users who have rated the test movie are extracted.  In this subset of users a similarity comparison is done against the test user.  A sorted list is returned.  From the most similar users in this list, as set by a parameter $k$, a rating can be made by averaging results of their scores.  The search is expensive because there is no easy way of visiting the entire user space, especially with a dense subset.  Because this technique compares user-to-user directly it receives the UBCF designation.

\subsection{IBCF}

Similar to UBCF, IBCF instead seeks items that are most similar to each other.  In most traditional uses of IBCF, a list of most similar items are returned based on a similarity score.  The algorithm has been modified to instead return a user's estimated rating from a similarity matrix.

The first step is to construct a user / movie ratings matrix where all ratings of the training set are filled accordingly.  Then, systematically, each item's ratings are compared to all other items.  A similarity computation is performed that yields a similarity number.  This number is stored in a user / movie similarity matrix.  Note that this matrix is triangular and will be roughly half the size of the ratings matrix.

When a test user requests a rating, the test movie's index is used.  Along this row are returned the similarity values for all of the test user's known ratings.  A weighted sum of the user's own ratings is used via the formula to construct the final result:

$$ \textrm{Rating} = \frac{\sum_{i=1}^k( r_i * sim_i )}{\sum_{i=1}^k(sim_i)} $$

Another weighing explored was the squared sum.  The highest similarity is always 1 therefore the lower the similarity the more the squared rating is penalized:

$$ \textrm{Rating} = \frac{\sum_{i=1}^k( r_i * sim_i^2 )}{\sum_{i=1}^k(sim_i^2)} $$


\subsection{Similarity Metrics:  Cosine and Pearson}

Similarity metrics follow two main formulations.  Cosine distance measures similarity via the formula:

$$ cos(\theta) = similarity(A,B) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^N A_i B_i}{\sqrt{\sum_{i=1}^N A_i^2}\sqrt{\sum_{i=1}^N B_i^2}} $$

Pearson similarity is computed as:

$$ r = similarity(A,B) = \frac{\sum_{i=1}^N(A_i - \bar{A})(B_i - \bar{B})}{\sqrt{\sum_{i=1}^N(A_i - \bar{A})^2}\sqrt{\sum_{i=1}^N(B_i - \bar{B})^2}}$$

Both forms will be examined across UBCF and IBCF.  The Pearson is expensive because it requires two passes of the data, one for the average, and the second for the similarity.  However, this cost may be justified because of an inherent tendency to normalize.

\subsection{Normalization}

Users can be optimists or pessimists on all their ratings or for portions of their ratings.  To see the effect of this bias, similarity measurements will be conducted under normalized and non-normalized data.  In UBCF two main biases will be normalized.  The first will be for computation of similarity.  The second will be on the returned score to the user, to shift back into the biased domain.

For IBCF, the similarity matrix is constructed with normalized ratings.  Because only a similarity is returned, a test user's bias need not be computed.

The formula normalization formula is simply subtracting the average of a user's ratings from each rating:

$$ Rating_i = Rating_{ui} - \overline{Rating_u} $$

\subsection{Blending of Global Data for Cold Starts or Sparse Matches}

In cases where similarity yields undefined results, for example, floating point overflow or underflow, the average rating for a movie is used instead.  Care is taken to check for such conditions.

\subsection{Other Approaches}

There are many other areas to explore with recommender systems.  Some mentioned in [2], [5], [6], and [7] include Restricted Boltzmann Machines (RBM), Matrix Factorization / Single Value Decomposition (SVD), and Slope-One estimation.  Even more exotic approaches described in [3] include simulated annealing.  Indeed, the space of applications is very large.  Computation and limited research time prevented exploration into these areas.  [1] indicates that the Netflix Prize \underline{intermediate} solution took 2000 hours and involved 107 algorithms for RBM and SVD, which exceeded the time alloted for the assignment.  The Slope-One estimation was more applicable with linear regressions and not with the implemented system and hence was not used.  Of the implemented collaborative filtering methods, the problem space was bounded at the highest density regions of data.  Such an approach could establish an upper bound to these algorithms which justifies this simplification.



%\subsection{Slope-one Estimation}
%
%Slope-one Estimation is another area to explore[5].  This technique simplifies UBCF and IBCF by forcing the linear approximators to use a slope of 1 against one free parameter.  This free parameter is computed by taking the average difference of ratings between two items or two users.  It is then added to the known rating to produce the targeted result.  Slope-one is advertised as a technique to reduce overfitting.

%The main objective is to predict how a given user would rate a movie based on the dataset.  To %establish a baseline, the commonly used technique is through collaborative filtering.  Because this %data set has high dimensionality, PCA or SVD will be used to reduce dimension.  A similarity metric of %cosine distance across the ratings will per user will be used.  RMSE error rates will be the final %metric of comparison on a dataset.  Cross-validation will be used for error measurements.
%
%The additional improvements in classification will come from evaluating weighing factors for time of %evaluation.  Several areas to consider are the effects of new users versus established raters, %frequency of ratings in time, time of year, time within a new release window, and release of movie %relative performance review.  Timing permitting additional blended / ensemble methods might be %employed such as gradient descent.  

%%Because I am new to many of these techniques, I request some flexibility in goals and data exploration techniques.


\section{Plan and Experiment}

\subsection{Dataset}

The dataset is comprised of over 100 million ratings of 17700 movies from 480,000 users.  17700 text files, one per movie, contain a user id, rating, and date per line in ASCII comma-separated format.  A movie ID value was given on the first line of the file as an integer number.  A separate file gives the movie title for a given ID.  User IDs were arbitrary integers assigned by Netflix that stripped away personally identifying information.  The same ID is used by a user for all movies that they rate.  Ratings are ordinal integer values from 1 to 5, with 1 indicating the worst rating and 5 the best.  Dates are also given in month-day-year format.  

The overall experiment will follow the guidelines for evaluation as set by Netflix.  A test set (not provided) will query the recommender system which will give a decimal rating for the user based on that user's prior movie ratings.  Netflix secretly held the actual rating given by the user.  Errors will be tabulated using the root-mean-square-error formula:

$$ RMSE = \sqrt{\frac{\sum_{i=1}^N (\hat{y_i} - y_i)^2}{N}} $$

-- Dataset overall description, explain how subsample density sets are generated

-- RMSE evaluation, chosen by Netflix over linear error methods, such as MAE, to make the penalty of errors be more pronounced.  Emphasizes accuracy.

-- Baseline is average ratings for each movie over the entire training set.  This is the standard used by the papers written for CF and for the Netflix Prize.  It is reasonable because these papers note at how well it works, even when some methods begin failing.

\subsection{Hypotheses / Questions}

-- Normalization will offer some benefit to both algorithms.  However, the effect should be relatively small.  The ratings space only occupies 5 ordinal values; there is not much to normalize.

-- High density data will yield subperb results for UBCF or IBCF.  These are the best conditions for such data.  However, a cross-over point will be observed where average RMSE will beat these metrics.  A simple test against a low density sample showed that RMSE scored 1.05 while the other methods ranged from 1.20 to 1.32 RMSE.  A cross-over point might occur in the 30 \% density region.

-- User based collaborative filtering will work better than item based collaborative filtering.  This is just a guess.  It will be interesting to see which method produces better results.

-- Pearson similarity will beat Cosine similarity.  If any normalization effects occur, Pearson gives some treatment to them because it computes similarity from the average.  Further, Pearson will have little effect between normalization and non-normalized data.

-- A relationship for a fixed number of movies and fixed number of users in a subsample will demonstrate which trend might hit a cross-over point sooner.

-- Low $k$ values in the k-nearest-neighbors parameters will perform poorly.  [7] notes that these metrics are somewhat arbitrary.

\subsection{Experimental Design}

\begin{table}[]
\centering
\caption{Movie vs. User Count High Density Subsamples}
\label{density-table-label}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Movie / User & 500   & 1500  & 2500  & 3500  & 4500  \\ \hline
1000         & 0.851 & 0.794 & 0.763 & 0.741 & 0.723 \\ \hline
3000         & 0.624 & 0.541 & 0.502 & 0.476 & 0.456 \\ \hline
5000         & 0.491 & 0.405 & 0.368 & 0.344 & 0.327 \\ \hline
7000         & 0.401 & 0.320 & 0.287 & 0.267 & 0.252 \\ \hline
9000         & 0.338 & 0.263 & 0.234 & 0.217 & 0.204 \\ \hline
\end{tabular}
\end{table}

-- Random number generator, repeatability

-- 10-fold cross validation

-- Tests generated in a fold by randomly removing a rating for a test user.  The rating was withheld from the CF algorithms and used in the RMSE calculations.

-- Subsampled dataset picked movies and users by number of ratings counts.

-- Density matrix established:  Show chart of Top User x Top Movie with density filled in

-- The first experiment will determine which k-NN is the best setting for each combination of input.  Results will be computed against the M1500\_U500 subsample with further subsamples depending on results.  The $k$ parameter will vary from 1 to 20 (or more or less) to find a maximized setting.  The results of these experiments will then set the k-value for all subsequent tests.

-- The remaining experiments will use the 5x5 set of high density subsamples found in table (entry here).  Cosine and Pearson will be applied across all IBCF / UBCF for RMSE evaluation.

-- Normalization settings will be added to the prior settings to their effects.  Normalization occurs in the similarity matrix construction and similarity evaluation in both CF settings.  It is also used to denormalize a user's bias un UBCF.

-- U and M parameters held while other parameter varies to uncover where IBCF and UBCF excel or underperform.

-- Across all experiments IBCF and UBCF compared.



While the data are considered clean by Netflix, it is enormous, occupying over 1.5 GB of space on 17700 separate text files.  Exploration of the space revealed that for many of the functions in {\it recommenderlab}, the memory constraints were insufficient to hold the data.  To get around this issue, a computer program was developed to sample recommendations from the overall merged dataset.  Parameters given include the number of users, number of movies, and random seed.  The program runs in two passes.  The first pass collects all of the users from the randomly chosen movies.  From this user list, random users are chosen up to the input parameter.  A file is then produced with these sampled selections.  The file is passed to another program which measures bulk statistics to ensure ratings follow a similar distribution as the overall dataset.  Additional user metrics are compared to ensure the number of user ratings scales with the diminished movie count.

From the sampled data set, RMSE will be computed with k=10 cross-validations (k value subject to change pending on runtime) against each global recommender method.  This procedure will be replicated across all valid sample set and serves as a replacement of Netflix's hidden testset.

A first priority is to establish rankings based on bulk metrics through average ratings.  This is the starting point for the Netflix prize contributions.  This technique yields 1.05475 RMSE, similar to other contestant's findings.

From here IBCF and UBCF models will be explored.  Parameters considered will be number of nearest neighbors (k-NN value), similarity metric (cosine vs. Pearson), normalization, recency of ratings, and blending of global movie parameters for sparse queries[3].  Time permitting, the Slope-one method of estimation will be explored.  If possible, a hybrid model will be constructed that blends various combinations of the above into an overall model.  Each technique is briefly discussed.


\section{Results}

-- knn graph of results with values chosen in the experiments, also shows IBCF better than UBCF

-- combined graphs for Pearson/Cosine/Denormalized/Normalized/Baseline per IBCF and UBCF

-- overall density vs. RMSE (demonstrates no cross-over point in the high density region)

-- UBCF and IBCF parametric graphs (hold one constant while other varies)
 

\section{Conclusion}

The dataset is large.  A good portion of time was spent on exploring IBCF and UBCF features in {\it recommenderlab} by using the built-in MovieLense database.  The routines did a great job in explaining the overall programming approach, but they fail to run against extraordinarily large amounts of data.  The remedy is to sample the full dataset with a collection of C++ tools.  To this end, much of the development work has gone into creating these tools and processing the data for use in R Studio.

The toolsuite might be extended to perform the calculations described in this report.  Raw C++ can handle the large amounts of data and run much quicker than R's interpreted environment.  However, the interpreted environment offers the advantage of operating on data without reloading it for each experiment.

A day or two might be spent looking for APIs to enhance the approach.


KNN IMPORTANCE
NORMALIZATION IMPORTANCE
OPTIMIZATION
GPU
IBCF BETTER THAN UBCF
DENSITY AS LOW AS 20\% WORK WELL POSSIBLY MUCH LOWER
INTERPOLATION METHODS

(SEE SLIDES)


%The formatting instructions contained in these style files are summarized in
%sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

%% \subsection{Keywords for paper submission}
%% Your NIPS paper can be submitted with any of the following keywords (more than one keyword is possible for each paper):

%% \begin{verbatim}
%% Bioinformatics
%% Biological Vision
%% Brain Imaging and Brain Computer Interfacing
%% Clustering
%% Cognitive Science
%% Control and Reinforcement Learning
%% Dimensionality Reduction and Manifolds
%% Feature Selection
%% Gaussian Processes
%% Graphical Models
%% Hardware Technologies
%% Kernels
%% Learning Theory
%% Machine Vision
%% Margins and Boosting
%% Neural Networks
%% Neuroscience
%% Other Algorithms and Architectures
%% Other Applications
%% Semi-supervised Learning
%% Speech and Signal Processing
%% Text and Language Applications

%% \end{verbatim}

% \section{General formatting instructions}
% \label{gen_inst}
% 
% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.
% 
% Paper title is 17~point, initial caps/lower case, bold, centered between
% 2~horizontal rules. Top rule is 4~points thick and bottom rule is 1~point
% thick. Allow 1/4~inch space above and below title to rules. All pages should
% start at 1~inch (6~picas) from the top of the page.
% 
% %The version of the paper submitted for review should have ``Anonymous Author(s)'' as the author of the paper.
% 
% For the final version, authors' names are
% set in boldface, and each name is centered above the corresponding
% address. The lead author's name is to be listed first (left-most), and
% the co-authors' names (if different address) are set to follow. If
% there is only one co-author, list both author and co-author side by side.
% 
% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.
% 
% \section{Headings: first level}
% \label{headings}
% 
% First level headings are lower case (except for first word and proper nouns),
% flush left, bold and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.
% 
% \subsection{Headings: second level}
% 
% Second level headings are lower case (except for first word and proper nouns),
% flush left, bold and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.
% 
% \subsubsection{Headings: third level}
% 
% Third level headings are lower case (except for first word and proper nouns),
% flush left, bold and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.
% 
% \section{Citations, figures, tables, references}
% \label{others}
% 
% These instructions apply to everyone, regardless of the formatter being used.
% 
% \subsection{Citations within the text}
% 
% Citations within the text should be numbered consecutively. The corresponding
% number is to appear enclosed in square brackets, such as [1] or [2]-[5]. The
% corresponding references are to be listed in the same order at the end of the
% paper, in the \textbf{References} section. (Note: the standard
% \textsc{Bib\TeX} style \texttt{unsrt} produces this.) As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.
% 
% As submission is double blind, refer to your own published work in the 
% third person. That is, use ``In the previous work of Jones et al.\ [4]'',
% not ``In our previous work [4]''. If you cite your other papers that
% are not widely available (e.g.\ a journal paper under review), use
% anonymous author names in the citation, e.g.\ an author of the
% form ``A.\ Anonymous''. 
% 
% 
% \subsection{Footnotes}
% 
% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}
% 
% \subsection{Figures}
% 
% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.
% 
% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.
% 
% You may use color figures. 
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}
% 
% \subsection{Tables}
% 
% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.
% 
% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.
% 
% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}
% 
% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.
% 
% \section{Preparing PostScript or PDF files}
% 
% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.
% 
% Fonts were the main cause of problems in the past years. Your PDF file must
% only contain Type 1 or Embedded TrueType fonts. Here are a few instructions
% to achieve this.
% 
% \begin{itemize}
% 
% \item You can check which fonts a PDF files uses.  In Acrobat Reader,
% select the menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
% also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
% available out-of-the-box on most Linux machines.
% 
% \item The IEEE has recommendations for generating PDF files whose fonts
% are also acceptable for NIPS. Please see
% \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}
% 
% \item LaTeX users:
% 
% \begin{itemize}
% 
% \item Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user). 
% PDF figures must be substituted for EPS figures, however.
% 
% \item Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim} 
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}
% 
% Check that the PDF files only contains Type 1 fonts. 
% %For the final version, please send us both the Postscript file and
% %the PDF file. 
% 
% \item xfig "patterned" shapes are implemented with 
% bitmap fonts.  Use "solid" shapes instead. 
% \item The \verb+\bbold+ package almost always uses bitmap
% fonts.  You can try the equivalent AMS Fonts with command
% \begin{verbatim}
% \usepackage[psamsfonts]{amssymb}
% \end{verbatim}
%  or use the following workaround for reals, natural and complex: 
% \begin{verbatim}
% \newcommand{\RR}{I\!\!R} %real numbers
% \newcommand{\Nat}{I\!\!N} %natural numbers 
% \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% 
% \item Sometimes the problematic fonts are used in figures
% included in LaTeX files. The ghostscript program \verb+eps2eps+ is the simplest
% way to clean such figures. For black and white figures, slightly better
% results can be achieved with program \verb+potrace+.
% \end{itemize}
% \item MSWord and Windows users (via PDF file):
% \begin{itemize}
% \item Install the Microsoft Save as PDF Office 2007 Add-in from
% \url{http://www.microsoft.com/downloads/details.aspx?displaylang=en\&familyid=4d951911-3e7e-4ae6-b059-a2e79ed87041}
% \item Select ``Save or Publish to PDF'' from the Office or File menu
% \end{itemize}
% \item MSWord and Mac OS X users (via PDF file):
% \begin{itemize}
% \item From the print menu, click the PDF drop-down box, and select ``Save
% as PDF...''
% \end{itemize}
% \item MSWord and Windows users (via PS file):
% \begin{itemize}
% \item To create a new printer
% on your computer, install the AdobePS printer driver and the Adobe Distiller PPD file from
% \url{http://www.adobe.com/support/downloads/detail.jsp?ftpID=204} {\it Note:} You must reboot your PC after installing the
% AdobePS driver for it to take effect.
% \item To produce the ps file, select ``Print'' from the MS app, choose
% the installed AdobePS printer, click on ``Properties'', click on ``Advanced.''
% \item Set ``TrueType Font'' to be ``Download as Softfont''
% \item Open the ``PostScript Options'' folder
% \item Select ``PostScript Output Option'' to be ``Optimize for Portability''
% \item Select ``TrueType Font Download Option'' to be ``Outline''
% \item Select ``Send PostScript Error Handler'' to be ``No''
% \item Click ``OK'' three times, print your file.
% \item Now, use Adobe Acrobat Distiller or ps2pdf to create a PDF file from
% the PS file. In Acrobat, check the option ``Embed all fonts'' if
% applicable.
% \end{itemize}
% 
% \end{itemize}
% If your file contains Type 3 fonts or non embedded TrueType fonts, we will
% ask you to fix it. 
% 
% \subsection{Margins in LaTeX}
%  
% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ... 
%    \includegraphics[width=0.8\linewidth]{myfile.eps} 
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ... 
%    \includegraphics[width=0.8\linewidth]{myfile.pdf} 
% \end{verbatim}
% for .pdf graphics. 
% See section 4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps}) 
%  
% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.
% 
% 
% \subsubsection*{Acknowledgments}
% 
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments go at the end of the paper. Do not include 
% acknowledgments in the anonymized submission, only in the 
% final paper. 
% 

\subsection*{GIT Repository}

Code for this project can be found under the 'c++' subdirectory at the Github address \url{https://github.com/AquaPi271/alda_project}.

\subsubsection*{References}

\small{
[1] Xavier Amatriain, Justin Basilico. {\it Netflix Recommendations: Beyond the 5 Stars (Two Parts)}.  The Netflix Tech Blog, 2012. https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429. 11/5/2017.

[2] Robert M. Bell, Yehuda Koren. (2007)  {\it Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights}.  Seventh IEEE International Conference on Data Mining. pp. 43-52. IEEE.

[3] Linyuan Lu, Matus Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, Tao Zhou. (2012) {\it Recommender systems}. In Physics Reports pp. 1-49, Elsevier B. V. 0370-1573.

[4] Michael Hahsler (2017). recommenderlab: Lab for Developing and Testing Recommender Algorithms. R package version 0.2-2. http://lyle.smu.edu/IDA/recommenderlab/

[5] Daniel Lemire, Anna Maclachlan. (2007). {\it Slope One Predictors for Online Rating-Based Collaborative Filtering}. In CoRR, Volume abs/cs/0702144.

[6] Joonseok Lee, Mingxuan Sun, Guy Lebanon. (2012).  A Comparative Study of Collaborative Filtering Algorithms.

[7] Yehuda Koren. (2009). The BellKor Solution to the Netflix Grand Prize.

}
% \subsubsection*{References}
% 
% References follow the acknowledgments. Use unnumbered third level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to `small' (9-point) 
% when listing the references. {\bf Remember that this year you can use
% a ninth page as long as it contains \emph{only} cited references.}
% 
% \small{
% [1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
% for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
% and T.K. Leen (eds.), {\it Advances in Neural Information Processing
% Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.
% 
% [2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
% Realistic Neural Models with the GEneral NEural SImulation System.}
% New York: TELOS/Springer-Verlag.
% 
% [3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
% and recall at excitatory recurrent synapses and cholinergic modulation
% in rat hippocampal region CA3. {\it Journal of Neuroscience}
% {\bf 15}(7):5249-5262.
% }

\end{document}
